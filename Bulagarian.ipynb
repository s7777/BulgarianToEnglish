{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L21h7_ksjgMI",
        "outputId": "ed9ce960-9581-4a4c-ed92-b6a235200c8f"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t') for line in  lines]\n",
        "\treturn pairs\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\t#re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\t\t\t# normalize unicode characters\n",
        "\t\t\t#line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\t\t#line = line.decode('UTF-8')\n",
        "\t\t\t# tokenize on white space\n",
        "\t\t\tline = line.split()\n",
        "\t\t\t# convert to lowercase\n",
        "\t\t\t#line = [word.lower() for word in line]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tline = [word.translate(table) for word in line]\n",
        "\t\t\t# remove non-printable chars form each token\n",
        "\t\t\t#line = [re_print.sub('', w) for w in line]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t#\tline = [word for word in line if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "filename = 'bul.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-Bulgarian.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-Bulgarian.pkl\n",
            "[Hi] => [Здрасти]\n",
            "[Hello] => [Здравейте]\n",
            "[Cheers] => [Наздраве]\n",
            "[Got it] => [Разбра ли]\n",
            "[Im OK] => [ОК съм]\n",
            "[Im OK] => [Наред съм]\n",
            "[No way] => [Абсурд]\n",
            "[No way] => [В никакъв случай]\n",
            "[Really] => [Сериозно]\n",
            "[Thanks] => [Мерси]\n",
            "[Ask Tom] => [Питай Том]\n",
            "[Ask Tom] => [Питайте Том]\n",
            "[Ask Tom] => [Попитайте Том]\n",
            "[Ask Tom] => [Попитай Том]\n",
            "[Ask Tom] => [Помоли Том]\n",
            "[Be cool] => [Дръж се като пич]\n",
            "[Be cool] => [Споко]\n",
            "[Get out] => [Излизай]\n",
            "[Get out] => [Излизайте]\n",
            "[Go away] => [Върви си]\n",
            "[Go away] => [Махай се]\n",
            "[Go away] => [Ходи си]\n",
            "[Go away] => [Вървете си]\n",
            "[Go away] => [Махайте се]\n",
            "[Go away] => [Ходете си]\n",
            "[Go away] => [Изчезвай]\n",
            "[Go away] => [Ходи си]\n",
            "[Go away] => [Върви си]\n",
            "[Go away] => [Махай се]\n",
            "[Go away] => [Махайте се]\n",
            "[Go away] => [Вървете си]\n",
            "[He runs] => [Бяга]\n",
            "[He runs] => [Тича]\n",
            "[Im sad] => [Тъжен съм]\n",
            "[Its OK] => [Всичко е наред]\n",
            "[Me too] => [И аз също]\n",
            "[Me too] => [И аз така]\n",
            "[Me too] => [Аз също]\n",
            "[See you] => [До скоро]\n",
            "[Show me] => [Покажи ми]\n",
            "[Welcome] => [Добре дошъл]\n",
            "[You run] => [Тичаш]\n",
            "[You run] => [Бягаш]\n",
            "[You run] => [Тичате]\n",
            "[You run] => [Бягате]\n",
            "[Get lost] => [Изчезвай]\n",
            "[Get lost] => [Изчезвайте]\n",
            "[Have fun] => [Забавлявай се]\n",
            "[Have fun] => [Забавлявайте се]\n",
            "[Hurry up] => [Побързай]\n",
            "[I forgot] => [Забравих]\n",
            "[I forgot] => [Аз забравих]\n",
            "[Ill pay] => [Ще платя]\n",
            "[Ill pay] => [Аз ще платя]\n",
            "[Im full] => [Наситих се]\n",
            "[Im full] => [Наядох се]\n",
            "[Im full] => [Пълен съм]\n",
            "[Im poor] => [Беден съм]\n",
            "[Im rich] => [Богат съм]\n",
            "[Its hot] => [Горещо е]\n",
            "[Lets go] => [Хайде]\n",
            "[Lets go] => [Хайде да тръгваме]\n",
            "[Lets go] => [Давай]\n",
            "[Lets go] => [Да вървим]\n",
            "[Lets go] => [Хайде]\n",
            "[Lets go] => [Хайде да тръгваме]\n",
            "[She runs] => [Бяга]\n",
            "[She runs] => [Тича]\n",
            "[Use this] => [Използвай това]\n",
            "[We agree] => [Съгласни сме]\n",
            "[Birds fly] => [Птиците летят]\n",
            "[Calm down] => [Успокой се]\n",
            "[Chill out] => [Успокой се]\n",
            "[Chill out] => [Успокой малко топката]\n",
            "[Do it now] => [Направи го сега]\n",
            "[Dont die] => [Не умирай]\n",
            "[Excuse me] => [Извини ме]\n",
            "[Excuse me] => [Извинете ме]\n",
            "[Excuse me] => [Извинявай]\n",
            "[Excuse me] => [Извинете]\n",
            "[Forget it] => [Забрави]\n",
            "[Go inside] => [Влез вътре]\n",
            "[Go inside] => [Прибери се]\n",
            "[Go inside] => [Влезте вътре]\n",
            "[I gave up] => [Отказах се]\n",
            "[I gave up] => [Аз се отказах]\n",
            "[I give in] => [Аз се отказвам]\n",
            "[I give up] => [Отказвам се]\n",
            "[I give up] => [Аз се предавам]\n",
            "[I laughed] => [Аз се изсмях]\n",
            "[I laughed] => [Аз се смеех]\n",
            "[I laughed] => [Смях се]\n",
            "[I laughed] => [Смеех се]\n",
            "[I laughed] => [Изсмях се]\n",
            "[I lost it] => [Изгубих го]\n",
            "[I lost it] => [Превъртях]\n",
            "[I met him] => [Видях го]\n",
            "[I met him] => [Срещнах се с него]\n",
            "[Im a man] => [Аз съм мъж]\n",
            "[Im early] => [Подранил съм]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePwndeGOki4Z",
        "outputId": "545978de-907e-4f56-981e-acc78c4855fa"
      },
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-Bulgarian.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 10000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:9000], dataset[1000:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-Bulgarian-both.pkl')\n",
        "save_clean_data(train, 'english-Bulgarian-train.pkl')\n",
        "save_clean_data(test, 'english-Bulgarian-test.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-Bulgarian-both.pkl\n",
            "Saved: english-Bulgarian-train.pkl\n",
            "Saved: english-Bulgarian-test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcRk9vBEkxBa",
        "outputId": "77e10ea5-70b2-40dd-d3ba-7cf083f15e43"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-Bulgarian-both.pkl')\n",
        "train = load_clean_sentences('english-Bulgarian-train.pkl')\n",
        "test = load_clean_sentences('english-Bulgarian-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Bulgarian Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Bulgarian Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 3029\n",
            "English Max Length: 9\n",
            "Bulgarian Vocabulary Size: 5866\n",
            "Bulgarian Max Length: 12\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 12, 256)           1501696   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 9, 256)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 9, 256)            525312    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 9, 3029)           778453    \n",
            "=================================================================\n",
            "Total params: 3,330,773\n",
            "Trainable params: 3,330,773\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "141/141 - 48s - loss: 3.9758 - val_loss: 3.1928\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.19280, saving model to model.h5\n",
            "Epoch 2/100\n",
            "141/141 - 41s - loss: 3.1171 - val_loss: 3.0215\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.19280 to 3.02146, saving model to model.h5\n",
            "Epoch 3/100\n",
            "141/141 - 41s - loss: 2.9893 - val_loss: 2.9112\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.02146 to 2.91121, saving model to model.h5\n",
            "Epoch 4/100\n",
            "141/141 - 41s - loss: 2.9010 - val_loss: 2.8438\n",
            "\n",
            "Epoch 00004: val_loss improved from 2.91121 to 2.84379, saving model to model.h5\n",
            "Epoch 5/100\n",
            "141/141 - 41s - loss: 2.8213 - val_loss: 2.7641\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.84379 to 2.76408, saving model to model.h5\n",
            "Epoch 6/100\n",
            "141/141 - 41s - loss: 2.7401 - val_loss: 2.6864\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.76408 to 2.68636, saving model to model.h5\n",
            "Epoch 7/100\n",
            "141/141 - 41s - loss: 2.6661 - val_loss: 2.6108\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.68636 to 2.61084, saving model to model.h5\n",
            "Epoch 8/100\n",
            "141/141 - 41s - loss: 2.5941 - val_loss: 2.5431\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.61084 to 2.54311, saving model to model.h5\n",
            "Epoch 9/100\n",
            "141/141 - 41s - loss: 2.5131 - val_loss: 2.4645\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.54311 to 2.46454, saving model to model.h5\n",
            "Epoch 10/100\n",
            "141/141 - 42s - loss: 2.4211 - val_loss: 2.3756\n",
            "\n",
            "Epoch 00010: val_loss improved from 2.46454 to 2.37556, saving model to model.h5\n",
            "Epoch 11/100\n",
            "141/141 - 42s - loss: 2.3321 - val_loss: 2.2859\n",
            "\n",
            "Epoch 00011: val_loss improved from 2.37556 to 2.28594, saving model to model.h5\n",
            "Epoch 12/100\n",
            "141/141 - 41s - loss: 2.2346 - val_loss: 2.1977\n",
            "\n",
            "Epoch 00012: val_loss improved from 2.28594 to 2.19775, saving model to model.h5\n",
            "Epoch 13/100\n",
            "141/141 - 41s - loss: 2.1352 - val_loss: 2.0899\n",
            "\n",
            "Epoch 00013: val_loss improved from 2.19775 to 2.08991, saving model to model.h5\n",
            "Epoch 14/100\n",
            "141/141 - 42s - loss: 2.0262 - val_loss: 1.9835\n",
            "\n",
            "Epoch 00014: val_loss improved from 2.08991 to 1.98349, saving model to model.h5\n",
            "Epoch 15/100\n",
            "141/141 - 41s - loss: 1.9226 - val_loss: 1.8840\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.98349 to 1.88399, saving model to model.h5\n",
            "Epoch 16/100\n",
            "141/141 - 41s - loss: 1.8186 - val_loss: 1.7830\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.88399 to 1.78303, saving model to model.h5\n",
            "Epoch 17/100\n",
            "141/141 - 41s - loss: 1.7166 - val_loss: 1.6859\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.78303 to 1.68587, saving model to model.h5\n",
            "Epoch 18/100\n",
            "141/141 - 41s - loss: 1.6158 - val_loss: 1.5966\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.68587 to 1.59664, saving model to model.h5\n",
            "Epoch 19/100\n",
            "141/141 - 41s - loss: 1.5208 - val_loss: 1.5176\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.59664 to 1.51756, saving model to model.h5\n",
            "Epoch 20/100\n",
            "141/141 - 41s - loss: 1.4317 - val_loss: 1.4321\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.51756 to 1.43207, saving model to model.h5\n",
            "Epoch 21/100\n",
            "141/141 - 41s - loss: 1.3450 - val_loss: 1.3619\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.43207 to 1.36189, saving model to model.h5\n",
            "Epoch 22/100\n",
            "141/141 - 41s - loss: 1.2649 - val_loss: 1.2764\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.36189 to 1.27642, saving model to model.h5\n",
            "Epoch 23/100\n",
            "141/141 - 40s - loss: 1.1862 - val_loss: 1.2157\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.27642 to 1.21568, saving model to model.h5\n",
            "Epoch 24/100\n",
            "141/141 - 41s - loss: 1.1128 - val_loss: 1.1480\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.21568 to 1.14799, saving model to model.h5\n",
            "Epoch 25/100\n",
            "141/141 - 40s - loss: 1.0399 - val_loss: 1.0844\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.14799 to 1.08442, saving model to model.h5\n",
            "Epoch 26/100\n",
            "141/141 - 40s - loss: 0.9704 - val_loss: 1.0282\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.08442 to 1.02824, saving model to model.h5\n",
            "Epoch 27/100\n",
            "141/141 - 40s - loss: 0.9077 - val_loss: 0.9839\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.02824 to 0.98386, saving model to model.h5\n",
            "Epoch 28/100\n",
            "141/141 - 41s - loss: 0.8485 - val_loss: 0.9221\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.98386 to 0.92210, saving model to model.h5\n",
            "Epoch 29/100\n",
            "141/141 - 41s - loss: 0.7905 - val_loss: 0.8744\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.92210 to 0.87442, saving model to model.h5\n",
            "Epoch 30/100\n",
            "141/141 - 41s - loss: 0.7372 - val_loss: 0.8310\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.87442 to 0.83099, saving model to model.h5\n",
            "Epoch 31/100\n",
            "141/141 - 41s - loss: 0.6849 - val_loss: 0.7900\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.83099 to 0.79004, saving model to model.h5\n",
            "Epoch 32/100\n",
            "141/141 - 41s - loss: 0.6392 - val_loss: 0.7515\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.79004 to 0.75152, saving model to model.h5\n",
            "Epoch 33/100\n",
            "141/141 - 41s - loss: 0.5958 - val_loss: 0.7105\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.75152 to 0.71048, saving model to model.h5\n",
            "Epoch 34/100\n",
            "141/141 - 41s - loss: 0.5526 - val_loss: 0.6791\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.71048 to 0.67906, saving model to model.h5\n",
            "Epoch 35/100\n",
            "141/141 - 41s - loss: 0.5132 - val_loss: 0.6423\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.67906 to 0.64227, saving model to model.h5\n",
            "Epoch 36/100\n",
            "141/141 - 41s - loss: 0.4725 - val_loss: 0.6215\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.64227 to 0.62151, saving model to model.h5\n",
            "Epoch 37/100\n",
            "141/141 - 41s - loss: 0.4390 - val_loss: 0.5819\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.62151 to 0.58188, saving model to model.h5\n",
            "Epoch 38/100\n",
            "141/141 - 40s - loss: 0.4087 - val_loss: 0.5588\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.58188 to 0.55878, saving model to model.h5\n",
            "Epoch 39/100\n",
            "141/141 - 41s - loss: 0.3783 - val_loss: 0.5375\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.55878 to 0.53752, saving model to model.h5\n",
            "Epoch 40/100\n",
            "141/141 - 41s - loss: 0.3499 - val_loss: 0.5171\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.53752 to 0.51710, saving model to model.h5\n",
            "Epoch 41/100\n",
            "141/141 - 41s - loss: 0.3244 - val_loss: 0.4909\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.51710 to 0.49088, saving model to model.h5\n",
            "Epoch 42/100\n",
            "141/141 - 41s - loss: 0.2968 - val_loss: 0.4730\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.49088 to 0.47302, saving model to model.h5\n",
            "Epoch 43/100\n",
            "141/141 - 41s - loss: 0.2752 - val_loss: 0.4564\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.47302 to 0.45635, saving model to model.h5\n",
            "Epoch 44/100\n",
            "141/141 - 42s - loss: 0.2589 - val_loss: 0.4446\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.45635 to 0.44459, saving model to model.h5\n",
            "Epoch 45/100\n",
            "141/141 - 42s - loss: 0.2397 - val_loss: 0.4276\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.44459 to 0.42759, saving model to model.h5\n",
            "Epoch 46/100\n",
            "141/141 - 41s - loss: 0.2197 - val_loss: 0.4176\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.42759 to 0.41760, saving model to model.h5\n",
            "Epoch 47/100\n",
            "141/141 - 41s - loss: 0.2018 - val_loss: 0.4022\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.41760 to 0.40225, saving model to model.h5\n",
            "Epoch 48/100\n",
            "141/141 - 41s - loss: 0.1865 - val_loss: 0.3898\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.40225 to 0.38983, saving model to model.h5\n",
            "Epoch 49/100\n",
            "141/141 - 41s - loss: 0.1747 - val_loss: 0.3821\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.38983 to 0.38212, saving model to model.h5\n",
            "Epoch 50/100\n",
            "141/141 - 42s - loss: 0.1601 - val_loss: 0.3748\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.38212 to 0.37477, saving model to model.h5\n",
            "Epoch 51/100\n",
            "141/141 - 42s - loss: 0.1504 - val_loss: 0.3653\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.37477 to 0.36528, saving model to model.h5\n",
            "Epoch 52/100\n",
            "141/141 - 43s - loss: 0.1407 - val_loss: 0.3561\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.36528 to 0.35612, saving model to model.h5\n",
            "Epoch 53/100\n",
            "141/141 - 41s - loss: 0.1277 - val_loss: 0.3479\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.35612 to 0.34786, saving model to model.h5\n",
            "Epoch 54/100\n",
            "141/141 - 41s - loss: 0.1196 - val_loss: 0.3445\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.34786 to 0.34453, saving model to model.h5\n",
            "Epoch 55/100\n",
            "141/141 - 41s - loss: 0.1118 - val_loss: 0.3366\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.34453 to 0.33661, saving model to model.h5\n",
            "Epoch 56/100\n",
            "141/141 - 41s - loss: 0.1040 - val_loss: 0.3328\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.33661 to 0.33278, saving model to model.h5\n",
            "Epoch 57/100\n",
            "141/141 - 41s - loss: 0.0972 - val_loss: 0.3285\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.33278 to 0.32850, saving model to model.h5\n",
            "Epoch 58/100\n",
            "141/141 - 41s - loss: 0.0903 - val_loss: 0.3209\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.32850 to 0.32089, saving model to model.h5\n",
            "Epoch 59/100\n",
            "141/141 - 41s - loss: 0.0844 - val_loss: 0.3262\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.32089\n",
            "Epoch 60/100\n",
            "141/141 - 41s - loss: 0.0815 - val_loss: 0.3188\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.32089 to 0.31882, saving model to model.h5\n",
            "Epoch 61/100\n",
            "141/141 - 40s - loss: 0.0765 - val_loss: 0.3144\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.31882 to 0.31438, saving model to model.h5\n",
            "Epoch 62/100\n",
            "141/141 - 40s - loss: 0.0704 - val_loss: 0.3098\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.31438 to 0.30984, saving model to model.h5\n",
            "Epoch 63/100\n",
            "141/141 - 41s - loss: 0.0634 - val_loss: 0.3073\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.30984 to 0.30730, saving model to model.h5\n",
            "Epoch 64/100\n",
            "141/141 - 41s - loss: 0.0598 - val_loss: 0.3053\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.30730 to 0.30533, saving model to model.h5\n",
            "Epoch 65/100\n",
            "141/141 - 41s - loss: 0.0583 - val_loss: 0.3043\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.30533 to 0.30435, saving model to model.h5\n",
            "Epoch 66/100\n",
            "141/141 - 40s - loss: 0.0582 - val_loss: 0.3114\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.30435\n",
            "Epoch 67/100\n",
            "141/141 - 41s - loss: 0.0601 - val_loss: 0.3149\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.30435\n",
            "Epoch 68/100\n",
            "141/141 - 41s - loss: 0.0638 - val_loss: 0.3151\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.30435\n",
            "Epoch 69/100\n",
            "141/141 - 41s - loss: 0.0669 - val_loss: 0.3157\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.30435\n",
            "Epoch 70/100\n",
            "141/141 - 40s - loss: 0.0647 - val_loss: 0.3161\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.30435\n",
            "Epoch 71/100\n",
            "141/141 - 41s - loss: 0.0588 - val_loss: 0.3042\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.30435 to 0.30419, saving model to model.h5\n",
            "Epoch 72/100\n",
            "141/141 - 41s - loss: 0.0527 - val_loss: 0.3031\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.30419 to 0.30314, saving model to model.h5\n",
            "Epoch 73/100\n",
            "141/141 - 41s - loss: 0.0454 - val_loss: 0.2957\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.30314 to 0.29568, saving model to model.h5\n",
            "Epoch 74/100\n",
            "141/141 - 41s - loss: 0.0391 - val_loss: 0.2915\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.29568 to 0.29147, saving model to model.h5\n",
            "Epoch 75/100\n",
            "141/141 - 41s - loss: 0.0356 - val_loss: 0.2909\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.29147 to 0.29087, saving model to model.h5\n",
            "Epoch 76/100\n",
            "141/141 - 41s - loss: 0.0335 - val_loss: 0.2900\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.29087 to 0.28995, saving model to model.h5\n",
            "Epoch 77/100\n",
            "141/141 - 41s - loss: 0.0319 - val_loss: 0.2910\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.28995\n",
            "Epoch 78/100\n",
            "141/141 - 41s - loss: 0.0315 - val_loss: 0.2913\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.28995\n",
            "Epoch 79/100\n",
            "141/141 - 40s - loss: 0.0306 - val_loss: 0.2922\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.28995\n",
            "Epoch 80/100\n",
            "141/141 - 41s - loss: 0.0306 - val_loss: 0.2927\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.28995\n",
            "Epoch 81/100\n",
            "141/141 - 41s - loss: 0.0306 - val_loss: 0.2940\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.28995\n",
            "Epoch 82/100\n",
            "141/141 - 41s - loss: 0.0325 - val_loss: 0.3020\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.28995\n",
            "Epoch 83/100\n",
            "141/141 - 41s - loss: 0.0495 - val_loss: 0.3552\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.28995\n",
            "Epoch 84/100\n",
            "141/141 - 40s - loss: 0.1191 - val_loss: 0.3748\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.28995\n",
            "Epoch 85/100\n",
            "141/141 - 40s - loss: 0.1103 - val_loss: 0.3307\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.28995\n",
            "Epoch 86/100\n",
            "141/141 - 40s - loss: 0.0645 - val_loss: 0.3030\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.28995\n",
            "Epoch 87/100\n",
            "141/141 - 41s - loss: 0.0399 - val_loss: 0.2946\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.28995\n",
            "Epoch 88/100\n",
            "141/141 - 40s - loss: 0.0301 - val_loss: 0.2904\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.28995\n",
            "Epoch 89/100\n",
            "141/141 - 41s - loss: 0.0262 - val_loss: 0.2886\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.28995 to 0.28864, saving model to model.h5\n",
            "Epoch 90/100\n",
            "141/141 - 40s - loss: 0.0251 - val_loss: 0.2896\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.28864\n",
            "Epoch 91/100\n",
            "141/141 - 41s - loss: 0.0244 - val_loss: 0.2888\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.28864\n",
            "Epoch 92/100\n",
            "141/141 - 40s - loss: 0.0237 - val_loss: 0.2897\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.28864\n",
            "Epoch 93/100\n",
            "141/141 - 41s - loss: 0.0235 - val_loss: 0.2889\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.28864\n",
            "Epoch 94/100\n",
            "141/141 - 41s - loss: 0.0236 - val_loss: 0.2906\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.28864\n",
            "Epoch 95/100\n",
            "141/141 - 40s - loss: 0.0235 - val_loss: 0.2902\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.28864\n",
            "Epoch 96/100\n",
            "141/141 - 40s - loss: 0.0236 - val_loss: 0.2908\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.28864\n",
            "Epoch 97/100\n",
            "141/141 - 41s - loss: 0.0233 - val_loss: 0.2910\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.28864\n",
            "Epoch 98/100\n",
            "141/141 - 40s - loss: 0.0229 - val_loss: 0.2928\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.28864\n",
            "Epoch 99/100\n",
            "141/141 - 41s - loss: 0.0232 - val_loss: 0.2915\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.28864\n",
            "Epoch 100/100\n",
            "141/141 - 40s - loss: 0.0235 - val_loss: 0.2947\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.28864\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fce37af0588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiF8NetelCuQ",
        "outputId": "1427a7e1-9d60-44fb-9143-daccb51969ac"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-Bulgarian-both.pkl')\n",
        "train = load_clean_sentences('english-Bulgarian-train.pkl')\n",
        "test = load_clean_sentences('english-Bulgarian-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[Прави каквото щеш], target=[Do as you like], predicted=[do as you like]\n",
            "src=[Кога ще заминеш ти], target=[When will you leave], predicted=[when will you leave]\n",
            "src=[Да се хващаме за работа], target=[Lets get cracking], predicted=[lets get cracking]\n",
            "src=[Дървото гори], target=[Wood burns], predicted=[wood burns]\n",
            "src=[И това съм виждала], target=[Ive seen that too], predicted=[ive seen that too]\n",
            "src=[Искаме го обратно], target=[We want it back], predicted=[we want it back]\n",
            "src=[Аз ще се заема с това], target=[Ill take care of it], predicted=[ill take care of it]\n",
            "src=[Аз съм опасна], target=[Im dangerous], predicted=[im dangerous]\n",
            "src=[Това е моето бебе], target=[This is my baby], predicted=[this is my baby]\n",
            "src=[Хубав ден е], target=[Its a nice day], predicted=[its a nice day]\n",
            "BLEU-1: 0.733361\n",
            "BLEU-2: 0.683382\n",
            "BLEU-3: 0.651297\n",
            "BLEU-4: 0.533296\n",
            "test\n",
            "src=[Чие е това палто], target=[Whose coat is this], predicted=[whose coat is this]\n",
            "src=[Това е нашето училище], target=[This is our school], predicted=[this is our school]\n",
            "src=[Облякох си сакото наопаки], target=[I put my coat on inside out], predicted=[i put my coat on inside out]\n",
            "src=[Той харесва много английския], target=[He likes English very much], predicted=[he likes english very much]\n",
            "src=[Аз предпочитам чая пред кафето], target=[I prefer tea to coffee], predicted=[i prefer tea to coffee]\n",
            "src=[Пази се от Том], target=[Be careful of Tom], predicted=[be careful of tom]\n",
            "src=[Още са вътре], target=[Theyre still inside], predicted=[theyre still inside]\n",
            "src=[Аз я закарах до града], target=[I gave her a lift to town], predicted=[i gave her a lift to town]\n",
            "src=[Том дори не е тук], target=[Tom isnt even here], predicted=[tom isnt even here]\n",
            "src=[Ние сме наистина заети], target=[Were really busy], predicted=[were really busy]\n",
            "BLEU-1: 0.689865\n",
            "BLEU-2: 0.636665\n",
            "BLEU-3: 0.608040\n",
            "BLEU-4: 0.493094\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqFihzv7Hksr",
        "outputId": "f12ec300-ad70-4745-cdde-47982bbd8e5e"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 100:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-Bulgarian-both.pkl')\n",
        "train = load_clean_sentences('english-Bulgarian-train.pkl')\n",
        "test = load_clean_sentences('english-Bulgarian-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[Прави каквото щеш], target=[Do as you like], predicted=[do as you like]\n",
            "src=[Кога ще заминеш ти], target=[When will you leave], predicted=[when will you leave]\n",
            "src=[Да се хващаме за работа], target=[Lets get cracking], predicted=[lets get cracking]\n",
            "src=[Дървото гори], target=[Wood burns], predicted=[wood burns]\n",
            "src=[И това съм виждала], target=[Ive seen that too], predicted=[ive seen that too]\n",
            "src=[Искаме го обратно], target=[We want it back], predicted=[we want it back]\n",
            "src=[Аз ще се заема с това], target=[Ill take care of it], predicted=[ill take care of it]\n",
            "src=[Аз съм опасна], target=[Im dangerous], predicted=[im dangerous]\n",
            "src=[Това е моето бебе], target=[This is my baby], predicted=[this is my baby]\n",
            "src=[Хубав ден е], target=[Its a nice day], predicted=[its a nice day]\n",
            "src=[Забравих номера ви], target=[I forgot your number], predicted=[i forgot your number]\n",
            "src=[Не ми достигат парите], target=[Im short of money], predicted=[im short of money]\n",
            "src=[Той ни излъга], target=[He lied to us], predicted=[he lied to us]\n",
            "src=[Кажи го ясно], target=[Say it clearly], predicted=[say it clearly]\n",
            "src=[Какви са вашите изводи], target=[What are your conclusions], predicted=[what are your conclusions]\n",
            "src=[Тази седмица съм ваканция], target=[I am on holiday this week], predicted=[i am on holiday this week]\n",
            "src=[Мисля че имахме късмет], target=[I think we were lucky], predicted=[i think we were lucky]\n",
            "src=[Играя една телевизионна игра], target=[Im playing a TV game], predicted=[im playing a tv game]\n",
            "src=[В коя страна си родена], target=[What country were you born in], predicted=[what country were you born in]\n",
            "src=[Том спи в стаята си], target=[Tom is asleep in his room], predicted=[tom is asleep in his room]\n",
            "src=[Липсвахте ми], target=[I missed you], predicted=[i missed you]\n",
            "src=[Желязното е полезен метал], target=[Iron is a useful metal], predicted=[iron is a useful metal]\n",
            "src=[Извинете имам молба], target=[Excuse me I have a request], predicted=[excuse me i have a request]\n",
            "src=[Извини ме], target=[Excuse me], predicted=[excuse me]\n",
            "src=[Така си и знаех], target=[I didnt think so], predicted=[i didnt think so]\n",
            "src=[Чух те как ги окуражаваше], target=[I heard you cheering], predicted=[i heard you cheering]\n",
            "src=[Може ли да отворя кутията], target=[May I open the box], predicted=[may i open the box]\n",
            "src=[Запомни да заключиш вратата], target=[Remember to lock the door], predicted=[remember to lock the door]\n",
            "src=[Ти психично болен ли си], target=[Are you psychotic], predicted=[are you psychotic]\n",
            "src=[Ние играхме за победа], target=[We played to win], predicted=[we played to win]\n",
            "src=[Това твоите неща ли са], target=[Are these your things], predicted=[are these your things]\n",
            "src=[Той извади паспорта си], target=[He took out his passport], predicted=[he took out his passport]\n",
            "src=[Лошо ли е], target=[Is it bad], predicted=[is it bad]\n",
            "src=[Том избягваше Мери], target=[Tom avoided Mary], predicted=[tom avoided mary]\n",
            "src=[Запътих се към парка], target=[I walked toward the park], predicted=[i walked toward the park]\n",
            "src=[Аз имам три малки деца], target=[I have three young kids], predicted=[i have three young kids]\n",
            "src=[Къде е вашият баща], target=[Where is your father], predicted=[wheres your father]\n",
            "src=[Как си тези дни], target=[How are you these days], predicted=[how are you these days]\n",
            "src=[Какво те пита Том], target=[What did Tom ask you], predicted=[what did tom ask you]\n",
            "src=[Трябва да предупредя Том], target=[Ive got to warn Tom], predicted=[ive got to warn tom]\n",
            "src=[Закъсня с три дена], target=[Youre three days late], predicted=[youre three days late]\n",
            "src=[Гледай си работата], target=[Mind your own business], predicted=[mind your own business]\n",
            "src=[Аз ще почистя], target=[Ill clean it up], predicted=[ill clean it up]\n",
            "src=[Може ли да те видя за момент], target=[Can I see you a moment], predicted=[can i see you a moment]\n",
            "src=[Той е хубав], target=[He is nice], predicted=[he is nice]\n",
            "src=[Том седна до прозореца], target=[Tom sat by the window], predicted=[tom sat by the window]\n",
            "src=[Беше доста хладно], target=[It was quite cold], predicted=[it was quite cold]\n",
            "src=[Беше недовършена], target=[It was incomplete], predicted=[it was incomplete]\n",
            "src=[Ние сме женени един за друг], target=[Were married to each other], predicted=[were married to each other]\n",
            "src=[Не ми пипай фотоапарата], target=[Leave my camera alone], predicted=[dont touch my camera]\n",
            "src=[Знаех си че ще дойдеш тук навреме], target=[I knew youd get here in time], predicted=[i knew youd get here in time]\n",
            "src=[Баща ми отиде за риба], target=[My father went fishing], predicted=[my father went fishing]\n",
            "src=[Защо толкова ми се спи], target=[Why am I so sleepy], predicted=[why am i so sleepy]\n",
            "src=[Дръж си устата затворена], target=[Keep your mouth shut], predicted=[keep your mouth shut]\n",
            "src=[Опасявам се че ще изгубим мача пак], target=[Im afraid well lose the game], predicted=[im afraid well lose the game]\n",
            "src=[Храната ни свършва], target=[Were running short of food], predicted=[were running short of food]\n",
            "src=[Не мисля че той ще дойде], target=[I dont think hell come], predicted=[i dont think hell come]\n",
            "src=[Ще го пробвам пак], target=[I will try it again], predicted=[i will try it again]\n",
            "src=[Можем да отворим прозореца], target=[We can open a window], predicted=[we can open a window]\n",
            "src=[Имаш ли пари в себе си], target=[Do you have any money with you], predicted=[do you have any money with you]\n",
            "src=[Той ще стигне до Хакодате довечера], target=[He will reach Hakodate tonight], predicted=[he will reach hakodate tonight]\n",
            "src=[Мисля че си съвършена], target=[I think youre perfect], predicted=[i think youre perfect]\n",
            "src=[Задръжте рестото], target=[Keep the change], predicted=[keep the change]\n",
            "src=[Напоследък цените са паднали], target=[Prices have dropped recently], predicted=[prices have dropped recently]\n",
            "src=[Обичам да пиша песни на френски], target=[I like to write songs in French], predicted=[i like to write songs in french]\n",
            "src=[Вратовръзката ми е оранжева], target=[My tie is orange], predicted=[my tie is orange]\n",
            "src=[Мълчи и слушай], target=[Shut up and listen], predicted=[shut up and listen]\n",
            "src=[Не те ли е срам], target=[Arent you ashamed], predicted=[arent you ashamed]\n",
            "src=[Страшно съм зает], target=[I have my hands full], predicted=[i have my hands full]\n",
            "src=[Том ми пострига косата], target=[Tom cut my hair], predicted=[tom cut my hair]\n",
            "src=[Трябва да изчакам обаждането на Том], target=[I have to wait for Toms call], predicted=[i have to wait for toms call]\n",
            "src=[Не съм толкова уморена], target=[Im not so tired], predicted=[im not so tired]\n",
            "src=[Том може да говори френски], target=[Tom is able to speak French], predicted=[tom is able to speak french]\n",
            "src=[Ще се видим скоро], target=[See you soon], predicted=[see you soon]\n",
            "src=[Гърция е древна страна], target=[Greece is an old country], predicted=[greece is an old country]\n",
            "src=[Том няма да се задържи тук толкова], target=[Tom wont be here that long], predicted=[tom wont be here that long]\n",
            "src=[Коледата идва], target=[Christmas is coming], predicted=[christmas is coming]\n",
            "src=[Виждала съм това и преди], target=[Ive seen that before], predicted=[ive seen that before]\n",
            "src=[Забравих да изключа лампите], target=[I forgot to turn off the light], predicted=[i forgot to turn off the light]\n",
            "src=[Тя ми го прошепна в ухото], target=[She whispered it in my ear], predicted=[she whispered it in my ear]\n",
            "src=[Дръпни се от компютъра], target=[Back away from the computer], predicted=[back away from the computer]\n",
            "src=[Търся Ви], target=[Im looking for you], predicted=[im looking for you]\n",
            "src=[Том не може да ни остави сега], target=[Tom cant leave us now], predicted=[tom cant leave us now]\n",
            "src=[Мисля че ще останеш доволен], target=[I think youll be pleased], predicted=[i think youll be pleased]\n",
            "src=[Трябва да отида да видя Том], target=[I have to go see Tom], predicted=[i have to go see tom]\n",
            "src=[Вие можете ли да дойдете утре], target=[Can you come tomorrow], predicted=[can you come tomorrow]\n",
            "src=[Правиш ли нещо специално], target=[Are you doing anything special], predicted=[are you doing anything special]\n",
            "src=[Какво донесе Том], target=[What did Tom bring], predicted=[what did tom bring]\n",
            "src=[Сигурнa съм че сте много заети], target=[Im sure youre very busy], predicted=[im sure youre very busy]\n",
            "src=[Дай му ги], target=[Give them to him], predicted=[give them to him]\n",
            "src=[Том е два пъти постар от Мери], target=[Tom is twice Marys age], predicted=[tom is twice marys age]\n",
            "src=[Моля ви прочетете го още веднъж], target=[Please read it once more], predicted=[please read it once more]\n",
            "src=[Трябва да знам истината], target=[I have to know the truth], predicted=[i have to know the truth]\n",
            "src=[Това е малко разфокусирано], target=[Thats a little out of focus], predicted=[thats a little out of focus]\n",
            "src=[Какво купи], target=[What did you buy], predicted=[what did you buy]\n",
            "src=[Гледай ме внимателно], target=[Watch me closely], predicted=[watch me closely]\n",
            "src=[Мисля че трябва да размислиш], target=[I think you should reconsider], predicted=[i think you should reconsider]\n",
            "src=[Потисната ли си], target=[Are you depressed], predicted=[are you depressed]\n",
            "src=[Да се срещнем на обичайното място], target=[Lets meet at the usual place], predicted=[lets meet at the usual place]\n",
            "src=[Музеят днес е затворен], target=[The museum is closed now], predicted=[the museum is closed now]\n",
            "BLEU-1: 0.733361\n",
            "BLEU-2: 0.683382\n",
            "BLEU-3: 0.651297\n",
            "BLEU-4: 0.533296\n",
            "test\n",
            "src=[Чие е това палто], target=[Whose coat is this], predicted=[whose coat is this]\n",
            "src=[Това е нашето училище], target=[This is our school], predicted=[this is our school]\n",
            "src=[Облякох си сакото наопаки], target=[I put my coat on inside out], predicted=[i put my coat on inside out]\n",
            "src=[Той харесва много английския], target=[He likes English very much], predicted=[he likes english very much]\n",
            "src=[Аз предпочитам чая пред кафето], target=[I prefer tea to coffee], predicted=[i prefer tea to coffee]\n",
            "src=[Пази се от Том], target=[Be careful of Tom], predicted=[be careful of tom]\n",
            "src=[Още са вътре], target=[Theyre still inside], predicted=[theyre still inside]\n",
            "src=[Аз я закарах до града], target=[I gave her a lift to town], predicted=[i gave her a lift to town]\n",
            "src=[Том дори не е тук], target=[Tom isnt even here], predicted=[tom isnt even here]\n",
            "src=[Ние сме наистина заети], target=[Were really busy], predicted=[were really busy]\n",
            "src=[Трябва да купя една], target=[I need to buy one], predicted=[i need to buy one]\n",
            "src=[Трябва да се хващам пак за работа], target=[I have to go back to work], predicted=[i have to go back to work]\n",
            "src=[На сладки ли ми мирише], target=[Do I smell cookies], predicted=[do i smell cookies]\n",
            "src=[Планът ти се провали], target=[Your plan failed], predicted=[your plan failed]\n",
            "src=[Мисля че трябва да кажем на Том], target=[I think we should tell Tom], predicted=[i think we should tell tom]\n",
            "src=[Да си поръчаме суши], target=[Lets have sushi], predicted=[lets have sushi]\n",
            "src=[Имам три избора], target=[I have three choices], predicted=[i have three choices]\n",
            "src=[Аз въобще не те разбирам], target=[I dont understand you at all], predicted=[i dont understand you at all]\n",
            "src=[Том не се отказа], target=[Tom didnt give up], predicted=[tom didnt give up]\n",
            "src=[Кой слуша Том], target=[Who listens to Tom], predicted=[who listens to tom]\n",
            "src=[Къде ми е униформата], target=[Wheres my uniform], predicted=[wheres my uniform]\n",
            "src=[Виждал съм това и преди], target=[Ive seen that before], predicted=[ive seen that before]\n",
            "src=[Пет галона нормален ако обичате], target=[Five gallons of regular please], predicted=[five gallons of regular please]\n",
            "src=[Остани с менТом], target=[Stay with me Tom], predicted=[stay with me tom]\n",
            "src=[Той ми е батко], target=[Hes my older brother], predicted=[hes my older brother]\n",
            "src=[Поряза ли се], target=[Are you cut], predicted=[are you cut]\n",
            "src=[Том ми каза да се срещна с него там], target=[Tom told me to meet him there], predicted=[tom told me to meet him there]\n",
            "src=[Къде е пощенската кутия], target=[Where is the mailbox], predicted=[where is the mailbox]\n",
            "src=[Мисля че трябва да излезете], target=[I think you should leave now], predicted=[i think you should leave now]\n",
            "src=[На Том му е трудно да стои мирно], target=[Tom has trouble standing still], predicted=[tom has trouble standing still]\n",
            "src=[Внимавай там горе], target=[Be careful up there], predicted=[be careful up there]\n",
            "src=[Търся си очилата], target=[I am looking for my glasses], predicted=[i am looking for my glasses]\n",
            "src=[Това не ви ли стига], target=[Isnt it enough for you], predicted=[isnt it enough for you]\n",
            "src=[Не бях съгласен], target=[I didnt think so], predicted=[i didnt think so]\n",
            "src=[Вие се шегувате], target=[Youre joking], predicted=[youre joking]\n",
            "src=[Знам защо го искаш], target=[I know why you want it], predicted=[i know why you want it]\n",
            "src=[Всичко което Том прави не ми харесва], target=[I dont like anything Tom does], predicted=[i dont like anything tom does]\n",
            "src=[Том има добри оценки по френски], target=[Tom got good grades in French], predicted=[tom got good grades in french]\n",
            "src=[Лятото свърши], target=[Summer has ended], predicted=[summer has ended]\n",
            "src=[Тази е твърде малка], target=[This is too small], predicted=[this is too small]\n",
            "src=[Чувам че ще се жениш], target=[I hear youre getting married], predicted=[i hear youre getting married]\n",
            "src=[Вие вярващи ли сте], target=[Are you a believer], predicted=[are you a believer]\n",
            "src=[Не съм Ви виждал от няколко дена], target=[I havent seen you in days], predicted=[i havent seen you in days]\n",
            "src=[Моля говорете повисоко], target=[Please speak more loudly], predicted=[please speak more loudly]\n",
            "src=[Какъв е адресът на Том], target=[Whats Toms address], predicted=[whats toms address]\n",
            "src=[Видя ли някой доктор], target=[Did you see a doctor], predicted=[did you see a doctor]\n",
            "src=[Вчера се видяхме с близнаците], target=[We saw the twins yesterday], predicted=[we saw the twins yesterday]\n",
            "src=[Не се изисква никакво разрешение], target=[No permit is required], predicted=[no permits is required]\n",
            "src=[За да продължите натиснете произволен клавиш], target=[Press any key to continue], predicted=[press any key to continue]\n",
            "src=[Невинаги съм си вкъщи в неделя], target=[Im not always home on Sundays], predicted=[im not always home on sundays]\n",
            "src=[Никъде няма да бъдеш защитена], target=[You wont be safe anywhere], predicted=[you wont be safe anywhere]\n",
            "src=[Той пренебрегна задълженията си], target=[He neglected his duties], predicted=[he neglected his duties]\n",
            "src=[Би ли ял това], target=[Would you eat this], predicted=[would you eat this]\n",
            "src=[Учила съм френски], target=[Ive studied French], predicted=[ive studied french]\n",
            "src=[Вярваш ли че има бог], target=[Do you believe in God], predicted=[do you believe in god]\n",
            "src=[Колко дълго остана], target=[How long did you stay], predicted=[how long did you stay]\n",
            "src=[Мисля че сме в беда], target=[I think were in trouble], predicted=[i think were in trouble]\n",
            "src=[Невинаги е лесно], target=[Its not always easy], predicted=[its not always easy]\n",
            "src=[Оттогава не съм разговаряла с Том], target=[I havent spoken to Tom since], predicted=[i havent spoken to tom since]\n",
            "src=[Предлагам да ядем], target=[We should eat], predicted=[we should eat]\n",
            "src=[Купих го], target=[I bought it], predicted=[i bought it]\n",
            "src=[Нямам нищо общо], target=[I have nothing to do with this], predicted=[i have nothing to do with this]\n",
            "src=[Моля те покажи ми как се прави], target=[Show me how to do it please], predicted=[show me how to do it please]\n",
            "src=[Мисля че за момента сме в безопасност], target=[I think were safe for a while], predicted=[i think were safe for a while]\n",
            "src=[С краката ми ли си играеш], target=[Are you playing with my feet], predicted=[are you playing with my feet]\n",
            "src=[Той се облегна на лактите си], target=[He leaned on his elbows], predicted=[he leaned on his elbows]\n",
            "src=[Може ли да свиря на пианото], target=[May I play the piano], predicted=[may i play the piano]\n",
            "src=[Ще бъда ок], target=[Ill be all right], predicted=[ill be all right]\n",
            "src=[Вината не е на Том], target=[Tom is not to blame], predicted=[tom is not to blame]\n",
            "src=[Ние учим френски], target=[We study French], predicted=[were learning french]\n",
            "src=[Трябва да чакам], target=[I have to wait], predicted=[i have to wait]\n",
            "src=[Ще дам всичко от себе си], target=[I will do my best], predicted=[i will do my best]\n",
            "src=[Тръгваш ли], target=[Are you leaving], predicted=[are you leaving]\n",
            "src=[Франция граничи с Италия], target=[France borders Italy], predicted=[france borders italy]\n",
            "src=[С мен не се консултираха], target=[I wasnt consulted], predicted=[i wasnt consulted]\n",
            "src=[Том работи извънредно], target=[Tom is working overtime], predicted=[tom is working overtime]\n",
            "src=[Не съм те виждала тук преди], target=[I havent seen you here before], predicted=[i havent seen you here before]\n",
            "src=[Мисля че това ще Ви хареса], target=[I think youd like that], predicted=[i think youd like that]\n",
            "src=[Извинете може ли да видя онази блуза], target=[May I see that blouse please], predicted=[may i see that blouse please]\n",
            "src=[Не мисля че Том е себичен], target=[I dont think Tom is selfish], predicted=[i dont think tom is selfish]\n",
            "src=[Научих доста], target=[I learned quite a bit], predicted=[i learned quite a bit]\n",
            "src=[Дай ми го], target=[Give it to me please], predicted=[give it to me please]\n",
            "src=[Много е кратко], target=[Its very short], predicted=[its very short]\n",
            "src=[Дай ми и малко мляко], target=[Give me some milk too], predicted=[give me some milk too]\n",
            "src=[Дайте ми мнението си моля], target=[Give me your opinion please], predicted=[give me your opinion please]\n",
            "src=[Мисля че това там е Том], target=[I think thats Tom over there], predicted=[i think thats tom over there]\n",
            "src=[Изиграх те], target=[I tricked you], predicted=[i tricked you]\n",
            "src=[Той ми беше водач], target=[He acted as my guide], predicted=[he acted as my guide]\n",
            "src=[Той прилича на баща си], target=[He looks like his father], predicted=[he looks like his father]\n",
            "src=[Той изкара 90 по английски], target=[He got 90 in English], predicted=[he got 90 in english]\n",
            "src=[Не ми остава много], target=[I dont have long], predicted=[i dont have long]\n",
            "src=[Том беше на абсолютно сигурно място], target=[Tom was perfectly safe], predicted=[tom was perfectly safe]\n",
            "src=[Просто искам това което си е мое], target=[All I want is whats mine], predicted=[all i want is whats mine]\n",
            "src=[Свалих си разни неща], target=[Ive downloaded some stuff], predicted=[ive downloaded some stuff]\n",
            "src=[Не познавам всички], target=[I dont know everybody], predicted=[i dont know everybody]\n",
            "src=[Виждал съм те с Том], target=[Ive seen you with Tom], predicted=[ive seen you with tom]\n",
            "src=[Не спирай да вярваш в мен], target=[Dont give up on me], predicted=[dont give up on me]\n",
            "src=[Къде е болката], target=[Where is the pain], predicted=[where is the pain]\n",
            "src=[Внимавай с това], target=[Be careful with that], predicted=[be careful with that]\n",
            "src=[Да вървим], target=[Lets go], predicted=[lets go]\n",
            "BLEU-1: 0.689865\n",
            "BLEU-2: 0.636665\n",
            "BLEU-3: 0.608040\n",
            "BLEU-4: 0.493094\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}